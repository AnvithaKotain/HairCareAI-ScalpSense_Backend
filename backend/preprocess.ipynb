{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths\n",
    "dataset_path = r\"D:\\Final project\\Scalp_Sense\\dataset\"\n",
    "output_path = os.path.join(os.getcwd(), \"processed_dataset\")\n",
    "splits = [\"training\", \"validation\", \"testing\"]\n",
    "genders = [\"male\", \"female\"]\n",
    "\n",
    "# Create split folders\n",
    "for split in splits:\n",
    "    os.makedirs(os.path.join(output_path, split), exist_ok=True)\n",
    "\n",
    "# Function to split and copy images\n",
    "def split_and_copy(images, src_path, dst_base_path, gender, category):\n",
    "    train_imgs, temp_imgs = train_test_split(images, test_size=0.3, random_state=42)\n",
    "    val_imgs, test_imgs = train_test_split(temp_imgs, test_size=0.5, random_state=42)\n",
    "    for img_set, split in zip([train_imgs, val_imgs, test_imgs], splits):\n",
    "        dest_path = os.path.join(dst_base_path, split, gender, category)\n",
    "        os.makedirs(dest_path, exist_ok=True)\n",
    "        for img in img_set:\n",
    "            shutil.copy(os.path.join(src_path, img), os.path.join(dest_path, img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing non-scalp images for male\n",
      "Processing stage stage 1 for male\n",
      "Processing stage stage 2 for male\n",
      "Processing stage stage 3 for male\n",
      "Processing stage stage 4 for male\n",
      "Processing stage stage 5 for male\n",
      "Processing stage stage 6 for male\n",
      "Processing non-scalp images for female\n",
      "Processing stage stage 1 for female\n",
      "Processing stage stage 2 for female\n",
      "Processing stage stage 3 for female\n",
      "Processing stage stage 4 for female\n",
      "Processing stage stage 5 for female\n",
      "Dataset split and copy complete.\n"
     ]
    }
   ],
   "source": [
    "# Main processing\n",
    "for gender in genders:\n",
    "    gender_path = os.path.join(dataset_path, gender)\n",
    "    for subfolder in os.listdir(gender_path):\n",
    "        subfolder_path = os.path.join(gender_path, subfolder)\n",
    "\n",
    "        if not os.path.isdir(subfolder_path):\n",
    "            continue\n",
    "\n",
    "        images = [f for f in os.listdir(subfolder_path) if not f.startswith('.')]\n",
    "        \n",
    "        if subfolder.lower() == \"non-scalp\":\n",
    "            print(f\"Processing non-scalp images for {gender}\")\n",
    "            split_and_copy(images, subfolder_path, output_path, gender, \"non-scalp\")\n",
    "        else:\n",
    "            print(f\"Processing stage {subfolder} for {gender}\")\n",
    "            split_and_copy(images, subfolder_path, output_path, gender, subfolder)\n",
    "\n",
    "print(\"Dataset split and copy complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking training set for corrupt or oversized images...\n",
      "Checking validation set for corrupt or oversized images...\n",
      "Checking testing set for corrupt or oversized images...\n",
      "Image compression and corruption check complete.\n"
     ]
    }
   ],
   "source": [
    "# Image compression and corruption check\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "MAX_WIDTH = 3000\n",
    "MAX_HEIGHT = 3000\n",
    "COMPRESSION_QUALITY = 85\n",
    "\n",
    "def compress_image(img_path):\n",
    "    try:\n",
    "        img = Image.open(img_path)\n",
    "        if img.size[0] > MAX_WIDTH or img.size[1] > MAX_HEIGHT:\n",
    "            img.thumbnail((MAX_WIDTH, MAX_HEIGHT))\n",
    "        img.save(img_path, \"JPEG\", quality=COMPRESSION_QUALITY)\n",
    "    except Exception as e:\n",
    "        print(f\"Error compressing {img_path}: {e}\")\n",
    "        os.remove(img_path)\n",
    "\n",
    "# Validate and compress\n",
    "for split in splits:\n",
    "    print(f\"Checking {split} set for corrupt or oversized images...\")\n",
    "    for gender in genders:\n",
    "        split_gender_path = os.path.join(output_path, split, gender)\n",
    "        if not os.path.exists(split_gender_path):\n",
    "            continue\n",
    "        for category in os.listdir(split_gender_path):\n",
    "            category_path = os.path.join(split_gender_path, category)\n",
    "            for img_name in os.listdir(category_path):\n",
    "                img_path = os.path.join(category_path, img_name)\n",
    "                if img_name.startswith('.'):\n",
    "                    continue\n",
    "                try:\n",
    "                    img = Image.open(img_path)\n",
    "                    img.verify()\n",
    "                    img.close()\n",
    "                    compress_image(img_path)\n",
    "                except (IOError, SyntaxError):\n",
    "                    print(f\"Removing corrupt image: {img_path}\")\n",
    "                    os.remove(img_path)\n",
    "\n",
    "print(\"Image compression and corruption check complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images (male): 4753\n",
      "Training images (female): 2952\n",
      "Validation images (male): 1015\n",
      "Validation images (female): 634\n",
      "Testing images (male): 1022\n",
      "Testing images (female): 637\n"
     ]
    }
   ],
   "source": [
    "# Function to count images in a given folder\n",
    "def count_images(folder):\n",
    "    count = 0\n",
    "    for root, _, files in os.walk(folder):\n",
    "        count += len(files)\n",
    "    return count\n",
    "\n",
    "# Count images in the train, val, and test directories for both men and women\n",
    "train_path_men = os.path.join(output_path, \"training\", \"male\")\n",
    "train_path_women = os.path.join(output_path, \"training\", \"female\")\n",
    "val_path_men = os.path.join(output_path, \"validation\", \"male\")\n",
    "val_path_women = os.path.join(output_path, \"validation\", \"female\")\n",
    "test_path_men = os.path.join(output_path, \"testing\", \"male\")\n",
    "test_path_women = os.path.join(output_path, \"testing\", \"female\")\n",
    "\n",
    "# Print the number of images in each category\n",
    "print(\"Training images (male):\", count_images(train_path_men))\n",
    "print(\"Training images (female):\", count_images(train_path_women))\n",
    "print(\"Validation images (male):\", count_images(val_path_men))\n",
    "print(\"Validation images (female):\", count_images(val_path_women))\n",
    "print(\"Testing images (male):\", count_images(test_path_men))\n",
    "print(\"Testing images (female):\", count_images(test_path_women))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
